{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lexical resources.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T70uyRKKuif"
      },
      "source": [
        "# **Some lexical resources you can use for LING226**\n",
        "In this notebook I read in and save a variety of different existing lexicons. You can read the basic description about them and then copy the code in case you would like to incorporate these resources into your final project. For the sake of simplicity, I have uploaded these resources to a github repository and read them in here, so you can do the same without having to manually download the data. \n",
        "\n",
        "I read each resource in as a dictionary and demontrate the keys/values for you. If you would prefer to download them yourself [you can grab them here](https://github.com/scs-vuw/LING226). But this notebook will also point you to the relevant sources to cite if you do use these resources. All of the resources I have here are for English, although some of these authors also provide data in other languages – you'll need to search that up yourself if you're interested in similar lexicons for languages other than English. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV4iPtyVXXQg"
      },
      "source": [
        "# make a helper function to create dictionaries. \n",
        "\n",
        "import requests\n",
        "\n",
        "def get_word_rating_resource(url):\n",
        "  \"\"\"helper function to get lexical resources for LING226 students\n",
        "  resources are hosted on github as .txt in the form of Word\\tValue\\n\n",
        "  \"\"\"\n",
        "  # read the raw text and split on newlines\n",
        "  raw = requests.get(url).text.split('\\n')\n",
        "  \n",
        "  # split each pair and convert value to rounded float\n",
        "  # the if statement is there to avoid indexing errors when a row in a resource doesn't have complete data\n",
        "  raw_list = [(pair.split('\\t')[0], round(float(pair.split('\\t')[1]), 3)) for pair in raw if len(pair.split('\\t')) == 2]\n",
        "\n",
        "  # create a dictionary and return it\n",
        "  return dict(raw_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX-5y_birw-Q"
      },
      "source": [
        "# **Word Frequency Values**\n",
        "\n",
        "Word frequency refers to how frequent a word appears in general usage. One simplistic analysis one can make is to associate less frequent words with more difficult vocabulary. There are a lot of different frequency lists out there. The resource I will give you here are the [SUBTLXus frequency measures.](https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/subtlexus)\n",
        "\n",
        "These frequency measures are taken from a large corpus of subtitles in television and movies. Using this method, the authors have created frequency lists for a variety of different languages, which you can explore on their website. \n",
        "\n",
        "I have only included the Log10WF, which is a normalized measure of frequency that takes into account the logarithmic distribution of words in corpora. A higher value means that the word is more frequent. Also, keep in mind this data includes both capitalized and lowercased versions of some words – this was because the authors wanted to take proper names into account.\n",
        "\n",
        "You would want to use frequency for any comparison of vocabulary between texts in English. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEwEA1wTsT7y"
      },
      "source": [
        "# load in frequency\n",
        "freq_url = 'https://raw.githubusercontent.com/scs-vuw/LING226/main/subtlxus_frequency.txt'\n",
        "freq_dict = get_word_rating_resource(freq_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wtxHm6ntPEE"
      },
      "source": [
        "# one of the most frequent words in the English language\n",
        "freq_dict['the']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUFuBMkbtdf2"
      },
      "source": [
        "# what about some lower frequency words?\n",
        "# (.477 is the lowest value in this resource)\n",
        "freq_dict['Tyrannosaur']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTe3G6v5u6ra"
      },
      "source": [
        "# **Age of acquisition**\n",
        "\n",
        "This is another measure of vocabulary complexity / sophistication. These values represent the average age where native English speakers think they first learned a particular word. A lower value means people believed they learned those words earlier in life, suggesting those words are more frequent, more concrete, and less sophisticated. \n",
        "\n",
        "You could include this in an analysis as a measure of the overall complexity or sophistication of vocabulary, but keep in mind it is only one dimension of this feature. \n",
        "\n",
        "You [can read the paper here.](https://link.springer.com/article/10.3758/s13428-012-0210-4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brrVwrx8vey8"
      },
      "source": [
        "# read in aoa data\n",
        "\n",
        "aoa_url = 'https://raw.githubusercontent.com/scs-vuw/LING226/main/AoA_Brysbart.txt'\n",
        "aoa_dict = get_word_rating_resource(aoa_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVkGc_FrvooX"
      },
      "source": [
        "aoa_dict.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au8lDLHJvroN"
      },
      "source": [
        "# which words do people say they learned, on average, after 20 years of age?\n",
        "for word in aoa_dict.keys():\n",
        "  if aoa_dict[word] > 20:\n",
        "    print(word, aoa_dict[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgV8QbqsW7uL"
      },
      "source": [
        "# **Concreteness**\n",
        "\n",
        "Word concreteness is a measure of how abstract or concrete the concept associated with a lexical item is. Words are not necessarily only concrete or only abstract, but rather slide along a scale. For example, the meaning of the noun \"tree\" is more concrete than the meaning of the noun \"peace.\" The list includes mostly single word items but also some compound, two word items. \n",
        "\n",
        "In general, words which are more concrete are more frequent and easier to learn, suggesting that language which is more concrete may be less complex. But this is not a hard and fast rule. Concreteness, AOA, and frequency are three measures you might want to consider as a union when investigating anything related to vocabulary complexity. \n",
        "\n",
        "This resource is a list of average concreteness ratings for 40,000 English words. [You can find the paper here.]('https://link.springer.com/article/10.3758/s13428-013-0403-5')\n",
        "\n",
        "Annotators from Amazon Mechnical Turk were asked to rate how concrete a word was on a scale of 1-5, with a 1 meaning abstract and a 5 meaning concrete. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrMEQREGZmrp"
      },
      "source": [
        "# create concreteness dictionary\n",
        "concrete_url = ('https://raw.githubusercontent.com/scs-vuw/LING226/main/concreteness.txt')\n",
        "\n",
        "concrete_dict = get_word_rating_resource(concrete_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BUF_oV-Z0H9"
      },
      "source": [
        "concrete_dict.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc9UJvs_Z1N4"
      },
      "source": [
        "# you can now access mean concreteness ratings\n",
        "for word in concrete_dict.keys():\n",
        "  # I put a lot of conditions here just to limit the output. there are a LOT of words in here :)\n",
        "  if concrete_dict[word] == 5 and ' ' not in word and len(word) > 10:\n",
        "    print(word, concrete_dict[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPhzTvw_aS8W"
      },
      "source": [
        "# **Semantic Diversity Ratings**\n",
        "\n",
        "This lexicon is a computationally derived measure of word ambiguity. The authors analysed 1000-word spans in the British National Corpus and calculated how likely any one particular word could appear in the spans. A word which has a higher likelihood of appearing will be more semantically diverse, meaning it can have more polysemous meanings and senses. A word with lower ratings means it should have a more specific meaning. \n",
        "\n",
        "[You can read the paper here.](https://link.springer.com/article/10.3758%2Fs13428-012-0278-x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASe7QzHcbdDr"
      },
      "source": [
        "semd_url = 'https://raw.githubusercontent.com/scs-vuw/LING226/main/semantic_diversity.txt'\n",
        "semd_dict = get_word_rating_resource(semd_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLYcFAGbbhWD"
      },
      "source": [
        "semd_dict.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW97YwWKdOOo"
      },
      "source": [
        "# what are some of the most restricted words in terms of the contexts they can appear in?\n",
        "for word in semd_dict.keys():\n",
        "  if semd_dict[word] < 0.2:\n",
        "    print(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OufczDkvK5cR"
      },
      "source": [
        "# **Humor Ratings**\n",
        "\n",
        "This is a list of humor ratings for almost 5000 English words. [You can read the paper here.](https://link.springer.com/article/10.3758%2Fs13428-017-0930-6) \n",
        "\n",
        "Basically, they asked ~800 people on Amazon Mechanical Turk to rate how humorous a word was using a 1-5 scale. The values in this lexicon are the mean humor ratings for each word, after the authors trimmed and cleaned the data.\n",
        "\n",
        "You could use this resource to see whether a certain text/genre uses words with higher or lower individual humor ratings, or if a text even has words which are deemed to be funny. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhPRG9F3X_D8"
      },
      "source": [
        "# create humor dictionary\n",
        "humor_url = ('https://raw.githubusercontent.com/scs-vuw/LING226/main/humor.txt')\n",
        "humor_dict = get_word_rating_resource(humor_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aODRoZtYIdS"
      },
      "source": [
        "humor_dict.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4heYMfaWLld"
      },
      "source": [
        "# you can now access mean humor ratings\n",
        "for word in humor_dict.keys():\n",
        "  if humor_dict[word] > 4:\n",
        "    print(word, humor_dict[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP5lYsXSdNxb"
      },
      "source": [
        "# **Emotion ratings**\n",
        "\n",
        "This one is a bit different because each word has ratings for multiple different emotion. This resouce is a series of words and whether or not they are associated with a particular emotion. The emotions include:\n",
        "\n",
        "- anger\n",
        "- anticipation\n",
        "- disgust \n",
        "- fear\n",
        "- joy\n",
        "- negative\n",
        "- positive \n",
        "- sadness\n",
        "- surprise\n",
        "- trust\n",
        "\n",
        "The values associated with the words in the dictionary are a 0 or 1. A 0 means the word does not have an association, whereas a 1 means it does. So it is binary - either or. With a resource like this, one would likely then want to see how many words with these associations a particular text has. Notice that it also has positive/negative, so in a way this can also be used as a sentiment analysis resource. \n",
        "\n",
        "This is just one of many resources provided [on this website.](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjufYXEbe_So"
      },
      "source": [
        "# get emotion resource and split on newlines\n",
        "emotion_url = 'https://raw.githubusercontent.com/scs-vuw/LING226/main/emotion_lexicon.txt'\n",
        "raw_emotion = requests.get(emotion_url).text.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1mTWxTLfa7i"
      },
      "source": [
        "# create a list, but this time of triples\n",
        "emotion_list = [(triple.split('\\t')[0], triple.split('\\t')[1], round(float(triple.split('\\t')[2]),2)) for triple in raw_emotion]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPYnwrxri9OQ"
      },
      "source": [
        "# create empty dictionary with defaultdict having another dictionary inside\n",
        "from collections import defaultdict\n",
        "emotion_dict = defaultdict(dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yUTGe6xi-fg"
      },
      "source": [
        "# add each entry to the new dictionary\n",
        "for triple in emotion_list:\n",
        "  word, category, value = triple\n",
        "  emotion_dict[word][category] = value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcBwjiXAjtZj"
      },
      "source": [
        "# you can now look up words for their associations \n",
        "emotion_dict['nepotism']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQcsirNRlcPm"
      },
      "source": [
        "# and you can index deeper to get specific categories\n",
        "emotion_dict['nepotism']['negative']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IutcNMBpliy"
      },
      "source": [
        "# **General Inquirer List**\n",
        "\n",
        "This is another lexicon which groups words into particular categories. The different categories are [explained here](http://www.wjh.harvard.edu/~inquirer/homecat.htm)\n",
        "\n",
        "There are a *lot* of categories and you should look through them in order to know if there is anything interesting in here for you. For example, this list includes words associated with different emotions, activities, feelings, etc. The lists work in that words thought to be included with those particular emotions / feelings are simply grouped in that list. \n",
        "\n",
        "So if a word is in the list, you can assume it is associated with that concept. You would use a resource like this to find how many words of a particular category exist in a text. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUKZ6twIp3Y3"
      },
      "source": [
        "# GI list\n",
        "gi_url = 'https://raw.githubusercontent.com/scs-vuw/LING226/main/inquirerbasic.txt'\n",
        "raw_gi = requests.get(gi_url).text.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuhUnfn2p-I5"
      },
      "source": [
        "gi_dict = dict()\n",
        "\n",
        "for category in raw_gi:\n",
        "  gi_dict[category.split('\\t')[0]] = category.split('\\t')[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br0Cy1nTqmDH"
      },
      "source": [
        "# look at the categories\n",
        "list(gi_dict.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNBX6lEgqzB-"
      },
      "source": [
        "# which words are associated with strength?\n",
        "gi_dict['Strong_GI']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJN_FcAyq8Y1"
      },
      "source": [
        "# which words are associated with persistance?\n",
        "gi_dict['Persist_GI']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1L-9SqPmEcK"
      },
      "source": [
        "# **AFINN Sentiment**\n",
        "\n",
        "Here is the AFINN resource from the sentiment analysis notebook (part of week 05), in case you wanted this one as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18nF7zxkn6Qn"
      },
      "source": [
        "# afinn url\n",
        "\n",
        "afinn_url = 'https://raw.githubusercontent.com/scs-vuw/LING226/main/AFINN-111.txt'\n",
        "afinn_dict = get_word_rating_resource(afinn_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DThM9VJ-oBjo"
      },
      "source": [
        "afinn_dict['reckless']"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}